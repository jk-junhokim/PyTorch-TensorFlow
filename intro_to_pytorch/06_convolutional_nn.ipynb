{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Convolutional Neural Network\n",
    "\n",
    "* convolutional layer\n",
    "* relu layer\n",
    "* pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# machine learning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# helper functions\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "# device\n",
    "device = \"cpu\"\n",
    "\n",
    "# print(torch.__version__)\n",
    "# print(np.__version__)\n",
    "# print(torchvision.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0 Setup\n",
    "\n",
    "* prepare & load data\n",
    "* import helper functions\n",
    "* create batches using data_loader\n",
    "* automate train, test, eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download...\n"
     ]
    }
   ],
   "source": [
    "# PREPARE & LOAD DATA\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "# DECLARE ESSENTIAL INFO\n",
    "class_names = train_data.classes\n",
    "class_to_idx = train_data.class_to_idx\n",
    "\n",
    "\"\"\"\n",
    "print(class_names)\n",
    "print(class_to_idx)\n",
    "print(train_data, \"\\n\", test_data)\n",
    "\"\"\"\n",
    "\n",
    "# SET BATCHES\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False)\n",
    "\n",
    "\"\"\"\n",
    "print(train_dataloader, \"\\n\", test_dataloader)\n",
    "\"\"\"\n",
    "\n",
    "# IMPORT HELPER FUNCTIONS\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "    print(\"helper_functions.py already exists, skipping download...\")\n",
    "else:\n",
    "    print(\"Downloading helper_functions.py\")\n",
    "    request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "    with open(\"helper_functions.py\", \"wb\") as f:\n",
    "        f.write(request.content)\n",
    "\n",
    "from helper_functions import accuracy_fn\n",
    "\n",
    "def print_train_time(start: float,\n",
    "                     end: float,\n",
    "                     device: torch.device=None):\n",
    "    \"\"\"\n",
    "    Prints difference between start adn end time.\n",
    "    \"\"\"\n",
    "\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device} : {total_time: .3f} seconds\")\n",
    "    return total_time\n",
    "\n",
    "\n",
    "# AUTOMIZE TRAIN/TEST/EVAL FUNCTIONS\n",
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device=device):\n",
    "    \"\"\"\n",
    "    Trains model based on train data from data_loader.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # put data on target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # make predictions\n",
    "        y_pred = model.forward(X)\n",
    "\n",
    "        # calculate & accumulate loss (wrongness)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss \n",
    "\n",
    "        # calculate & accumulate acc\n",
    "        train_acc += accuracy_fn(y_true=y,\n",
    "                                 y_pred=y_pred.argmax(dim=1)) # change logits -> prediction labels\n",
    "\n",
    "        # optimize model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # keep track of progress\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(data_loader.dataset)} samples.\")\n",
    "    \n",
    "\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.4f} | Train acc: {train_acc:.2f}%\")\n",
    "\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               accuracy_fn,\n",
    "               device: torch.device=device):\n",
    "    \"\"\"\n",
    "    Performs a test loop with model going over\n",
    "    test data from data_loader.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            test_pred = model.forward(X)\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "\n",
    "def eval_model(model: torch.nn.Module,\n",
    "            data_loader: torch.utils.data.DataLoader,\n",
    "            loss_fn: torch.nn.Module,\n",
    "            accuracy_fn):\n",
    "    \"\"\"\n",
    "    Returns a dictionary containing the results of model.\n",
    "    Built for easy comparison between different models.\n",
    "    The core function is equivalent to that of the \"test_step\" function.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    loss, acc = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X, y in tqdm(data_loader):\n",
    "            # make predictions\n",
    "            y_pred = model.forward(X)\n",
    "\n",
    "            # accumulate loss & acc per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y,\n",
    "                                y_pred=y_pred.argmax(dim=1))\n",
    "            \n",
    "        # scale loss and acc to find average per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "\n",
    "    return {\"model_name\": model.__class__.__name__,\n",
    "            \"model_loss\": loss.item(), \n",
    "            \"model_acc\": acc}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.0 CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTModelCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture taht replicates TinyVGG model.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=input_shape,\n",
    "                                  out_channels=hidden_units,\n",
    "                                  kernel_size=3,\n",
    "                                  stride=1,\n",
    "                                  padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(in_channels=hidden_units,\n",
    "                                  out_channels=hidden_units,\n",
    "                                  kernel_size=3,\n",
    "                                  stride=1,\n",
    "                                  padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=hidden_units,\n",
    "                                  out_channels=hidden_units,\n",
    "                                  kernel_size=3,\n",
    "                                  stride=1,\n",
    "                                  padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(in_channels=hidden_units,\n",
    "                                  out_channels=hidden_units,\n",
    "                                  kernel_size=3,\n",
    "                                  stride=1,\n",
    "                                  padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2) \n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(in_features=hidden_units*7*7,\n",
    "                        out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    # print shape to calculate in_features for nn.Linear()\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        print(f\"Output shape of conv_block_1: {x.shape}\")\n",
    "        x = self.conv_block_2(x)\n",
    "        print(f\"Output shape of conv_block_2: {x.shape}\")\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model_cnn_1 = FashionMNISTModelCNN(input_shape=1,\n",
    "                                   hidden_units=10,\n",
    "                                   output_shape=len(class_names)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape of conv_block_1: torch.Size([1, 10, 14, 14])\n",
      "Output shape of conv_block_2: torch.Size([1, 10, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0366, -0.0940,  0.0686, -0.0485,  0.0068,  0.0290,  0.0132,  0.0084,\n",
       "         -0.0030, -0.0185]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIND IN_FEATURES THROUGH PRINTING SHAPES\n",
    "rand_image_tensor = torch.randn(size=(1, 28, 28))\n",
    "\n",
    "\"\"\"\n",
    "# model_cnn_1(rand_image_tensor.unsqueeze(dim=0))\n",
    "\n",
    "rand_image_tensor = torch.randn(size=(1, 10, 7, 7))\n",
    "print(rand_image_tensor.size())\n",
    "\n",
    "flatten_tensor = nn.Flatten()\n",
    "rand_image_flattened = flatten_tensor(rand_image_tensor)\n",
    "print(rand_image_flattened.size())\n",
    "\n",
    "# print(f\"Original image shape: {rand_image_tensor.shape}\")\n",
    "# print(f\"Unsqueezed image shape: {rand_image_tensor.unsqueeze(dim=0).shape}\")\n",
    "\"\"\"\n",
    "\n",
    "model_cnn_1(rand_image_tensor.unsqueeze(dim=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Exploring `nn.Conv2d()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"Image batch shape: {images.shape}\")\\nprint(f\"Single image shape: {test_image.shape}\")\\nprint(f\"Test image: \\n {test_image}\")\\n'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dummy data\n",
    "torch.manual_seed(42)\n",
    "\n",
    "images = torch.randn(size=(32, 3, 64, 64))\n",
    "test_image = images[0]\n",
    "\n",
    "\"\"\"\n",
    "print(f\"Image batch shape: {images.shape}\")\n",
    "print(f\"Single image shape: {test_image.shape}\")\n",
    "print(f\"Test image: \\n {test_image}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cnn_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64]) \n",
      " torch.Size([1, 10, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# create single conv2d layer\n",
    "torch.manual_seed(42)\n",
    "conv_layer = nn.Conv2d(in_channels=3,\n",
    "                       out_channels=10,\n",
    "                       kernel_size=(3,3),\n",
    "                       stride=1,\n",
    "                       padding=1)\n",
    "\n",
    "test_image = test_image.unsqueeze(dim=0)\n",
    "conv_output = conv_layer(test_image)\n",
    "print(test_image.shape, \"\\n\", conv_output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Exploring `nn.nn.MaxPool2d()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test image original shape: torch.Size([3, 64, 64])\n",
      "Test image with unsqueezed dimension: torch.Size([1, 3, 64, 64])\n",
      "Shape after going through conv layer: torch.Size([1, 10, 64, 64])\n",
      "Shape after going through conv and max pool layer: torch.Size([1, 10, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "test_image = images[1]\n",
    "\n",
    "print(f\"Test image original shape: {test_image.shape}\")\n",
    "print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n",
    "\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\n",
    "print(f\"Shape after going through conv layer: {test_image_through_conv.shape}\")\n",
    "\n",
    "test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\n",
    "print(f\"Shape after going through conv and max pool layer: {test_image_through_conv_and_max_pool.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
